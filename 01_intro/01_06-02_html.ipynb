{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "210870de-b1d8-4c75-b2c9-57877c87ce70",
   "metadata": {},
   "source": [
    "# Εισαγωγή στην HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba0a7db-3563-4542-b312-a0b6acb6020e",
   "metadata": {},
   "source": [
    "## Εισαγωγή"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b59ad74-ec41-4b3b-8465-218e4ef0ff49",
   "metadata": {},
   "source": [
    "Σε αυτό το κεφάλαιο, θα μάθουμε για το web scraping (συλλογή δεδομένων από τον ιστό), μια από τις πιο ζωτικές δεξιότητες για έναν ερευνητή στις ψηφιακές ανθρωπιστικές επιστήμες. Το **Web scraping** είναι η διαδικασία κατά την οποία αυτοματοποιούμε την κλήση ενός server (που φιλοξενεί έναν ιστότοπο) και την ανάλυση αυτής της αίτησης που είναι ένα αρχείο HTML. Η **HTML** σημαίνει HyperText Markup Language (Γλώσσα Σήμανσης Υπερκειμένου). Είναι ο τρόπος με τον οποίο δομούνται οι ιστότοποι. Όταν κάνουμε scraping σε έναν ιστότοπο, γράφουμε κανόνες για την εξαγωγή κομματιών πληροφοριών από αυτόν με βάση το πώς αυτά τα δεδομένα είναι δομημένα μέσα στην HTML. Για να είμαστε ικανοί στο web scraping, επομένως, πρέπει να μπορούμε να κατανοούμε και να αναλύουμε την HTML.\n",
    "\n",
    "Σε αυτή την ενότητα, θα αναλύσουμε την HTML και θα μάθετε τα πιο συνηθισμένα tags (ετικέτες), όπως τα div, p, strong και span. Θα μάθετε επίσης για τα attributes (χαρακτηριστικά) μέσα σε αυτά τα tags, όπως τα href, class και id. Είναι ζωτικής σημασίας να κατανοήσετε αυτό πριν προχωρήσετε στο επόμενο κεφάλαιο στο οποίο μαθαίνουμε πώς να κάνουμε web scraping με την Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b63b13-214f-45af-8cea-1ee9e9d02482",
   "metadata": {},
   "source": [
    "## Εμβάθυνση στην HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebb20af-21b9-4a0e-aa31-6c74c1deb04b",
   "metadata": {},
   "source": [
    "Γιατί λοιπόν είναι χρήσιμη η HTML; Η HTML, όπως και άλλες γλώσσες σήμανσης, όπως η **XML**, ή eXstensible Markup Language (Επεκτάσιμη Γλώσσα Σήμανσης), επιτρέπει στους χρήστες να δομήσουν δεδομένα μέσα σε δεδομένα. Αυτό επιτυγχάνεται με αυτό που είναι γνωστό ως tags (ετικέτες). Νομίζω ότι είναι καλύτερο να δούμε πώς φαίνεται αυτό στην πράξη. Ας εξετάσουμε ένα απλό αρχείο HTML.\n",
    "\n",
    "```\n",
    "<div>\n",
    "    <p>This is a paragraph</p>\n",
    "</div>\n",
    "```\n",
    "\n",
    "Παραπάνω, βλέπουμε μια πολύ απλή δομή αρχείου HTML. Στην πρώτη γραμμή αυτού του HTML, βλέπουμε `<div>`. Παρατηρήστε τη χρήση των `<` και `>`. Το άνοιγμα `<` υποδεικνύει την έναρξη ενός tag στην HTML. Ένα **tag** είναι ένας τρόπος να υποδηλώσουμε τη δομή μέσα σε ένα αρχείο HTML. Είναι ένας τρόπος να πούμε ότι αυτό που έρχεται μετά από αυτό το ένθετο κομμάτι κώδικα είναι αυτός ο τύπος δεδομένων. Μετά το `<`, βλέπουμε τη λέξη `div`. Αυτή η λέξη υποδηλώνει τον τύπο του tag που χρησιμοποιούμε. Σε αυτήν την περίπτωση, δημιουργούμε ένα `div` tag. Αυτός είναι ένας από τους πιο συνηθισμένους τύπους tags στην HTML. Μετά το όνομα του tag, βλέπουμε `>`. Αυτό προσδιορίζει το τέλος της δημιουργίας του tag.\n",
    "\n",
    "Στη γραμμή 2, βλέπουμε ένα ένθετο, ή με εσοχή κομμάτι HTML. Στην HTML, σε αντίθεση με την Python, η εσοχή είναι προαιρετική. Ωστόσο, είναι καλή πρακτική να χρησιμοποιούμε αλλαγές γραμμής και εσοχή στην HTML για να κάνουμε το έγγραφο πιο εύκολο στην ανάλυση για τους ανθρώπους. Η γραμμή δύο αρχίζει με ένα `p` tag. Το `p` tag στην HTML χρησιμοποιείται για να υποδηλώσει την αρχή μιας παραγράφου.\n",
    "\n",
    "Μετά τη δημιουργία του `p tag`, βλέπουμε `This is a paragraph` (Αυτή είναι μια παράγραφος). Στην HTML, το κείμενο που βρίσκεται έξω από τα tags είναι κείμενο που εμφανίζεται στην ιστοσελίδα. Σε αυτήν την περίπτωση, το αρχείο HTML θα εμφάνιζε το κείμενο `This is a paragraph`. Αμέσως μετά από αυτό το κομμάτι κειμένου συναντάμε το πρώτο μας close tag (ετικέτα κλεισίματος). Ένα `close tag` στην HTML υποδεικνύει ότι αυτό το ένθετο κομμάτι δομής έχει τελειώσει. Στην περίπτωσή μας, το πρώτο close tag είναι `</p>`. Ξέρουμε ότι είναι ένα close tag λόγω του `</`, σε αντίθεση με το `<`.\n",
    "\n",
    "Στην τελευταία μας γραμμή, βλέπουμε ένα close `div` tag."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054e050d-b0e2-4d8f-8304-df6982a493bc",
   "metadata": {},
   "source": [
    "## Κατανόηση των Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc45809-dcfd-45b5-bc9f-24097828ce36",
   "metadata": {},
   "source": [
    "Ας ρίξουμε μια ματιά σε ένα άλλο μπλοκ HTML. Αυτή τη φορά, θα κάνουμε μια μικρή αλλαγή. Μπορείτε να την εντοπίσετε;\n",
    "\n",
    "```\n",
    "<div class=\"content\">\n",
    "    <p>This is a paragraph</p>\n",
    "</div>\n",
    "```\n",
    "\n",
    "Εάν είπατε το τμήμα `class=\"content\"` του open div tag, τότε θα είχατε δίκιο. Αυτό το κομμάτι είναι ενσωματωμένο μέσα στο tag και είναι γνωστό ως **attribute** (χαρακτηριστικό). Στην περίπτωσή μας, το ειδικό attribute που χρησιμοποιείται είναι ένα `class` attribute (ένας πολύ συνηθισμένος τύπος attribute). Αυτό το attribute έχει μια τιμή `content`.\n",
    "\n",
    "Όταν κάνετε scraping σε ιστότοπους, μπορείτε να χρησιμοποιήσετε αυτά τα attributes για να καθορίσετε ποιο div tag θα πιάσετε. Υπάρχουν αρκετά συνηθισμένα attributes, συγκεκριμένα τα `class` και `id`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f9b071-55f4-4ad5-bb72-02ebe495248e",
   "metadata": {},
   "source": [
    "## Ανάλυση HTML με την BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d186f8d9-8075-4e50-a844-b04f9b0d8623",
   "metadata": {},
   "source": [
    "Τώρα που κατανοούμε λίγο για την HTML, ας αρχίσουμε να προσπαθούμε να την αναλύσουμε στην Python. Όταν **αναλύουμε** (parse) HTML, προσπαθούμε να αυτοματοποιήσουμε την αναγνώριση της δομής της HTML και να την ερμηνεύσουμε συστηματικά. Αυτή είναι η βάση για το web scraping. Για να αρχίσουμε, ας δημιουργήσουμε ένα απλό αρχείο HTML στη μνήμη."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cda73f06-d7c5-4c3f-b014-f7f3c7fe2560",
   "metadata": {},
   "outputs": [],
   "source": [
    "html = \"\"\"\n",
    "<html>\n",
    "    <body>\n",
    "        <div class=\"content\">\n",
    "            <p>This is our first content</p>\n",
    "        </div>\n",
    "        <div class=\"other\">\n",
    "            <p>This is is another piece of content</p>\n",
    "        </div>\n",
    "    </body>\n",
    "</html>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65943aff-6a1f-4064-92b5-6fbe00a14e43",
   "metadata": {},
   "source": [
    "Υπάρχουν πολλές βιβλιοθήκες Python διαθέσιμες για την ανάλυση δεδομένων HTML. Για πιο ισχυρά προβλήματα, το [Selenium](https://selenium-python.readthedocs.io) είναι το βιομηχανικό πρότυπο. Ενώ το Selenium είναι ισχυρό, έχει μια απότομη καμπύλη μάθησης και μπορεί να είναι προκλητικό για εκείνους που είναι νέοι στην Python, ειδικά για εκείνους που προγραμματίζουν στα Windows. Επιπλέον, τα περισσότερα προβλήματα web scraping μπορούν να λυθούν χωρίς το Selenium.\n",
    "\n",
    "Για αυτούς τους λόγους, δεν θα χρησιμοποιήσουμε το Selenium σε αυτό το εγχειρίδιο, αλλά την [BeautifulSoup](https://pypi.org/project/beautifulsoup4/). Η BeautifulSoup είναι μια ελαφριά βιβλιοθήκη Python για την ανάλυση HTML. Είναι γρήγορη και αποτελεσματική. Το πιο δύσκολο πράγμα σχετικά με την BeautifulSoup είναι να θυμάστε πώς να την εγκαταστήσετε και να την εισάγετε στην Python.\n",
    "\n",
    "Για να εγκαταστήσετε την BeautifulSoup, πρέπει να εκτελέσετε την ακόλουθη εντολή στο terminal σας:\n",
    "\n",
    "```\n",
    "pip install beautifulsoup4\n",
    "```\n",
    "\n",
    "Μόλις εγκατασταθεί, μπορείτε στη συνέχεια να εισάγετε την BeautifulSoup με τον ακόλουθο τρόπο:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4f6d217b-e204-4be6-bdf4-d0381d0211c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585725ac-e316-45ad-b176-4da2dc26aa92",
   "metadata": {},
   "source": [
    "Με την BeautifulSoup εισαγμένη, μπορούμε να χρησιμοποιήσουμε την κλάση `BeautifulSoup`. Αυτή η κλάση μας επιτρέπει να αναλύουμε HTML. Όπως θα δούμε σε όλο αυτό το κεφάλαιο, σπάνια θα έχετε HTML ως συμβολοσειρά μέσα στο Python script σας, αλλά προς το παρόν, καθώς ξεκινάμε, ας προσπαθήσουμε να αναλύσουμε την παραπάνω συμβολοσειρά `html` περνώντας την στην κλάση BeautifulSoup. Είναι Pythonic να ονομάζετε τη μεταβλητή BeautifulSoup σας `soup`. Εάν έχετε ένα πιο περίπλοκο script που αναλύει soup objects από πολλαπλούς ιστότοπους, μπορεί να θέλετε να είστε πιο πρωτότυποι στις συμβάσεις ονομασίας σας, αλλά για τους σκοπούς μας, το `soup` θα μας εξυπηρετήσει καλά."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6aeb540b-dd99-42c4-9fe9-d942a39445f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f87fec-e941-415b-8c9c-e2460faf4e14",
   "metadata": {},
   "source": [
    "Με το soup object μας δημιουργημένο στη μνήμη, μπορούμε τώρα να αρχίσουμε να το εξετάζουμε. Εάν το εκτυπώσουμε, δεν θα παρατηρήσουμε κάτι ιδιαίτερο γι' αυτό. Φαίνεται σαν μια κανονική συμβολοσειρά."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30d72710-f8db-43f9-a81d-1590cf40ea4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      "<body>\n",
      "<div class=\"content\">\n",
      "<p>This is our first content</p>\n",
      "</div>\n",
      "<div class=\"other\">\n",
      "<p>This is is another piece of content</p>\n",
      "</div>\n",
      "</body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e0dcc5-6677-48e7-8a63-37270b5eafa3",
   "metadata": {},
   "source": [
    "Ενώ μπορεί να μοιάζει με συμβολοσειρά, δεν είναι. Μπορούμε να το παρατηρήσουμε αυτό ρωτώντας την Python τι τύπος αντικειμένου είναι με τη συνάρτηση `type`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aedc53d8-c730-42e4-96ea-47134906f9c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.BeautifulSoup"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c62ef7-22fe-4a68-9a37-69cf0c663cd0",
   "metadata": {},
   "source": [
    "Όπως μπορούμε να δούμε, αυτό είναι μια κλάση bs4.BeautifulSoup. Αυτό σημαίνει ότι ενώ μπορεί να μοιάζει με συμβολοσειρά, στην πραγματικότητα περιέχει περισσότερα δεδομένα στα οποία μπορεί να γίνει πρόσβαση. Για παράδειγμα, μπορούμε να χρησιμοποιήσουμε τη μέθοδο `.find` για να βρούμε συγκεκριμένα την πρώτη εμφάνιση ενός συγκεκριμένου tag. Η μέθοδος `find` έχει ένα υποχρεωτικό όρισμα, μια συμβολοσειρά που θα είναι το όνομα του tag που επιθυμείτε να εξαγάγετε. Ας πιάσουμε το πρώτο `div` tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0942f3e2-11cf-4b1f-87df-a7e0201a6e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div class=\"content\">\n",
      "<p>This is our first content</p>\n",
      "</div>\n"
     ]
    }
   ],
   "source": [
    "first_div = soup.find(\"div\")\n",
    "print(first_div)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ec5583-2afc-4f0d-96fa-46b186aa2ef1",
   "metadata": {},
   "source": [
    "Όπως μπορείτε να δείτε, καταφέραμε να πιάσουμε το πρώτο `div` tag με το `.find`, αλλά ξέρουμε ότι υπάρχουν πολλαπλά `div` tags στη συμβολοσειρά HTML μας. Τι θα γινόταν αν θέλαμε να τα πιάσουμε όλα; Γι' αυτό, μπορούμε να χρησιμοποιήσουμε τη μέθοδο `.find_all`. Όπως το `.find`, το `.find_all` παίρνει ένα μόνο υποχρεωτικό όρισμα. Και πάλι, είναι η συμβολοσειρά του ονόματος του tag που επιθυμείτε να εξαγάγετε."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bae6a79a-120d-4383-8896-a4f27bcb1435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<div class=\"content\">\n",
      "<p>This is our first content</p>\n",
      "</div>, <div class=\"other\">\n",
      "<p>This is is another piece of content</p>\n",
      "</div>]\n"
     ]
    }
   ],
   "source": [
    "all_divs = soup.find_all(\"div\")\n",
    "print(all_divs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e4d602-0865-4154-9ff4-55df4351d43b",
   "metadata": {},
   "source": [
    "Σε αντίθεση με το `.find` που μας επιστρέφει ένα μόνο στοιχείο, η μέθοδος `.find_all` επιστρέφει μια λίστα tags. Τι θα γινόταν αν δεν θέλαμε όλα τα tags; Τι θα γινόταν αν θέλαμε μόνο να πιάσουμε τα `div` tags με ένα συγκεκριμένο class attribute. Η BeautifulSoup μας επιτρέπει να το κάνουμε αυτό περνώντας ένα δεύτερο όρισμα στο `.find` ή `.find_all`. Αυτό το όρισμα θα είναι ένα λεξικό του οποίου τα κλειδιά θα είναι τα attributes και οι τιμές του θα είναι τα ονόματα των attributes των tags που επιθυμείτε να εξαγάγετε."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "198c29dd-ef8c-42fd-91b6-1c46ce74baaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<div class=\"other\">\n",
      "<p>This is is another piece of content</p>\n",
      "</div>]\n"
     ]
    }
   ],
   "source": [
    "div_other = soup.find_all(\"div\", {\"class\": \"other\"})\n",
    "print(div_other)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c9cb7f-7049-44e0-b2f3-859a968053a1",
   "metadata": {},
   "source": [
    "Μόλις αποκτήσουμε το συγκεκριμένο tag που θέλουμε να εξαγάγουμε από την HTML, μπορούμε στη συνέχεια να έχουμε πρόσβαση στα ένθετα στοιχεία του. Κάθε αντικείμενο `bs4.BeautifulSoup` λειτουργεί με τον ίδιο τρόπο όπως το αρχικό soup. Περιέχει όλα τα children (ένθετα) tags. Μπορούμε, επομένως, να πιάσουμε το `p` tag που είναι ενσωματωμένο μέσα στο `div` tag που έχει ένα `class` attribute του `other` χρησιμοποιώντας το `.find(\"p\")`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ae8de037-74f0-464a-8553-9ddedf04850a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p>This is is another piece of content</p>\n"
     ]
    }
   ],
   "source": [
    "paragraph = div_other[0].find(\"p\")\n",
    "print(paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72bff36-7d55-49c9-a299-ca216387d0ec",
   "metadata": {},
   "source": [
    "Εάν εργαζόμαστε με κειμενικά δεδομένα, ωστόσο, σπάνια θέλουμε να διατηρήσουμε την HTML, αλλά θέλουμε να εξαγάγουμε το κείμενο μέσα στην HTML. Γι' αυτό, μπορούμε να έχουμε πρόσβαση στο ακατέργαστο κείμενο που βρίσκεται μέσα στην HTML με το `.text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b31fe9a2-7781-44ee-a024-39002d767d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is is another piece of content\n"
     ]
    }
   ],
   "source": [
    "print(paragraph.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc361f7-2b3b-4df5-964f-add6e5f8036f",
   "metadata": {},
   "source": [
    "Το να μπορούμε να το κάνουμε αυτό προγραμματιστικά σημαίνει ότι μπορούμε να αυτοματοποιήσουμε το scraping αρχείων HTML μέσω Python. Μπορούμε, για παράδειγμα, να εξαγάγουμε όλο το κείμενο από κάθε `div` tag στο αρχείο μας μέσω των ακόλουθων δύο γραμμών κώδικα."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f71bc91a-12bb-419b-8dad-2257f61eb114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "This is our first content\n",
      "\n",
      "\n",
      "This is is another piece of content\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for div in all_divs:\n",
    "    print(div.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e763b26-d076-4221-b167-c3c359fa3976",
   "metadata": {},
   "source": [
    "## Πώς να Βρείτε την HTML ενός Ιστότοπου"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c136fe-2f1e-467c-af34-96fe45aacdfd",
   "metadata": {},
   "source": [
    "Τώρα που είστε εξοικειωμένοι γενικά με την HTML και πώς λειτουργεί, ας βουτήξουμε και ας ρίξουμε μια ματιά σε κάποια πραγματική HTML από έναν πραγματικό ιστότοπο. Στο επόμενο κεφάλαιο, θα κάνουμε web scraping στη Wikipedia, οπότε ας επικεντρωθούμε στη Wikipedia και εδώ. Εάν χρησιμοποιείτε έναν web browser που το υποστηρίζει (Chrome και Firefox), μπορείτε να εισέλθετε σε λειτουργία προγραμματιστή (developer mode). Κάθε λειτουργικό σύστημα και browser έχει διαφορετικό σύνολο πλήκτρων συντόμευσης για να το κάνετε αυτό, αλλά σε όλους μπορείτε να κάνετε δεξί κλικ στην ιστοσελίδα και να κάνετε κλικ στο \"inspect\". Αυτό θα ανοίξει τη λειτουργία προγραμματιστή. Σε αυτό το σημείο του κεφαλαίου, συνιστώ ιδιαίτερα να μεταβείτε στο βίντεο καθώς θα είναι λίγο πιο εύκολο να το ακολουθήσετε.\n",
    "\n",
    "Για αυτό το κεφάλαιο (και το επόμενο), θα εργαστούμε συγκεκριμένα με αυτήν τη σελίδα:\n",
    "https://en.wikipedia.org/wiki/List_of_French_monarchs\n",
    "\n",
    "Προχωρήστε και ανοίξτε την σε μια άλλη οθόνη ή σε μια νέα καρτέλα. Αυτό το άρθρο της Wikipedia περιέχει κάποιο κείμενο, αλλά κυρίως φιλοξενεί μια λίστα όλων των Γάλλων μοναρχών, από τους Καρολιδίους μέχρι τα μέσα του 19ου αιώνα με τον Ναπολέοντα Γ'.\n",
    "\n",
    "Όταν επιθεωρείτε αυτή τη σελίδα, θα δείτε όλη την ένθετη HTML μέσα σε αυτήν. Αφιερώστε λίγο χρόνο και εξετάστε αυτά τα tags. Στην επόμενη ενότητα, θα μάθουμε πώς να κάνουμε scraping σε αυτή τη σελίδα, αλλά προς το παρόν ρίξτε μια ματιά σε αυτό που μπορούμε να κάνουμε με μερικές βασικές εντολές στην Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3554f4b6-92e9-4709-8d03-82258b2dee6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The monarchs of the Kingdom of France ruled from the establishment of the Kingdom of the West Franks in 843 until the fall of the Second French Empire in 1870, with several interruptions. Between the period from King Charles the Bald in 843 to King Louis XVI in 1792, France had 45 kings. Adding the 7 emperors and kings after the French Revolution, this comes to a total of 52 monarchs of France.\n",
      "\n",
      "In August 843 the Treaty of Verdun divided the Frankish realm into three kingdoms, one of which (Middle Francia) was short-lived; the other two evolved into France (West Francia) and, eventually, Germany (East Francia). By this time, the eastern and western parts of the land had already developed different languages and cultures.\n",
      "\n",
      "Initially, the kingdom was ruled primarily by two dynasties, the Carolingians and the Robertians, who alternated rule from 843 until 987, when Hugh Capet, the progenitor of the Capetian dynasty, took the throne.  The kings use the title \"King of the Franks\" until the late twelfth century; the first to adopt the title of \"King of France\" was Philip II (r. 1180–1223).  The Capetians ruled continuously from 987 to 1792 and again from 1814 to 1848. The branches of the dynasty which ruled after 1328, however, are generally given the specific branch names of Valois (until 1589), Bourbon (from 1589 until 1792 and from 1814 until 1830), and the Orléans (from 1830 until 1848).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_French_monarchs\"\n",
    "s = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(s.content)\n",
    "body = soup.find(\"div\", {\"id\": \"mw-content-text\"})\n",
    "for paragraph in body.find_all(\"p\")[:5]:\n",
    "    if paragraph.text.strip() != \"\":\n",
    "        print (paragraph.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7c0842-3877-4b5f-a297-a1017acb7c4c",
   "metadata": {},
   "source": [
    "Στο παραπάνω κελί, χρησιμοποιήσαμε δύο βιβλιοθήκες, τις requests και BeautifulSoup για να καλέσουμε τον διακομιστή Wikipedia που φιλοξενεί αυτήν τη συγκεκριμένη σελίδα. Στη συνέχεια αναζητήσαμε το div tag που περιέχει το κύριο σώμα της σελίδας. Σε αυτήν την περίπτωση, ήταν ένα div tag του οποίου το attribute \"id\" αντιστοιχούσε στο \"mw-content-text\". Στη συνέχεια αναζήτησα όλα τα \"p\" tags, ή παραγράφους μέσα σε αυτό το σώμα και εκτύπωσα το κείμενο εάν το κείμενο δεν ήταν κενό. Μέχρι το τέλος του επόμενου κεφαλαίου, όχι μόνο θα κατανοήσετε τον παραπάνω κώδικα, αλλά θα μπορείτε να τον γράψετε και να τον κωδικοποιήσετε μόνοι σας. Προς το παρόν, επιθεωρήστε αυτή τη σελίδα και δείτε αν μπορείτε να βρείτε πού βρίσκεται το div tag του οποίου το id αντιστοιχεί στο \"mw-content-text\" στην HTML. Είναι εντάξει αν αυτό είναι δύσκολο! Δεν είναι κάτι που μπορείτε να κάνετε γρήγορα φυσικά. Χρειάζεται εξάσκηση. Ένα κόλπο για να σας βοηθήσει να ξεκινήσετε, ωστόσο, είναι να κάνετε δεξί κλικ στην περιοχή που θέλετε να κάνετε scraping και στη συνέχεια να κάνετε κλικ στο inspect.\n",
    "\n",
    "Μόλις αισθανθείτε άνετα με αυτό, μη διστάσετε να προχωρήσετε στο επόμενο κεφάλαιο για να αρχίσετε να μαθαίνετε πώς να κάνετε web scraping!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb314d49-7da3-4aea-8133-46db2aea02e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
